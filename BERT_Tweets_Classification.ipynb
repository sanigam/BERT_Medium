{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1.0 in /opt/conda/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.18.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (3.12.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.30.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (47.3.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.18.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.6.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: tensorflow_hub in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub) (1.18.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.4.0->tensorflow_hub) (47.3.1)\n",
      "Requirement already satisfied: bert-for-tf2 in /opt/conda/lib/python3.7/site-packages (0.14.4)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.48.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.85)\n",
      "Tensorflow Version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "#Note if you are using google colab - please go to Runtime -> Change runtime type  and select GPU as Hardware accelerator. This will make notebook run faster.\n",
    "#github link: https://github.com/sanigam/BERT_Medium\n",
    "\n",
    "#Install following libraries before first run. For subsequent runs, you may comment these\n",
    "!pip install tensorflow==2.1.0\n",
    "!pip install tensorflow_hub\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "\n",
    "#Importing Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "from bert import bert_tokenization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Tensorflow version\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (7613, 2) \n"
     ]
    }
   ],
   "source": [
    "#Loading Tweets Data, you can load from file in your computer or from github link. \n",
    "#For a different use case, you can load different file in similar format\n",
    "\n",
    "#df = pd.read_csv('tweets.csv') #From your computer\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/sanigam/BERT_Medium/master/tweets.csv') #From github link\n",
    "\n",
    "\n",
    "print ( f'Data Shape: {df.shape} ') #Number of rows and column in data-frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4150</th>\n",
       "      <td>Owner of Chicago-Area Gay Bar Admits to Arson ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>@CIA hey you guy's i stopped a massacre so you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "4150  Owner of Chicago-Area Gay Bar Admits to Arson ...       0\n",
       "5118  @CIA hey you guy's i stopped a massacre so you...       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Non Disaster tweet (target = 0):\n",
      "-------------------------------\n",
      "Listen to this hit song. A summer Bomb full of positive energy and youth\n",
      "Did you like it?\n",
      "https://t.co/2LiWkJybE9 \n",
      "#Norge2040\n",
      "\n",
      "Sample Tweet indicating Disaster (target = 1):\n",
      "--------------------------------------\n",
      "Still can't get over the thunderstorm/tornado we were woken up to yesterday. Half the street is still in the dark! http://t.co/Y8h5v1j2y7\n",
      "\n",
      "Tweets distribution for Disaster Tweets (1)  and Non-Disaster Tweets (0)\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Optional Step: Looking into data\n",
    "display(df.sample(2)) #Sample rows of dataframe\n",
    "\n",
    "print ( '\\nSample Non Disaster tweet (target = 0):\\n-------------------------------')\n",
    "print ( df[df['target']==0].text.values[0] )\n",
    "\n",
    "print ( '\\nSample Tweet indicating Disaster (target = 1):\\n--------------------------------------')\n",
    "print ( df[df['target']==1].text.values[0] )\n",
    "\n",
    "print ( '\\nTweets distribution for Disaster Tweets (1)  and Non-Disaster Tweets (0)\\n------------------------------------------------------------------------')\n",
    "df['target'].hist() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (5100, 2)  ,  Test Data Shape:  (2513, 2)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into test and training\n",
    "df_train, df_test = train_test_split( df , test_size=0.33, random_state=42)\n",
    "\n",
    "print( f'Training Data Shape: {df_train.shape}  ,  Test Data Shape:  {df_test.shape}') # Rows/Cols in train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of vocab in our tokenizer :  30522\n"
     ]
    }
   ],
   "source": [
    "#Loading BERT Standard model (Pretrained Model on Wikipedia and Book Corpus)\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True, name = 'keras_bert_layer' )\n",
    "\n",
    "#Getting vocab file from bert layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() \n",
    "\n",
    "do_lower_case = True  # For uncased model it will True \n",
    "\n",
    "#Defining tokenizer object which will be used to tokenize text before feeding to bert\n",
    "tokenizer_for_bert = bert_tokenization.FullTokenizer(vocab_file, do_lower_case) #Tokenizer to tokenize input text\n",
    "\n",
    "print ( '\\nLength of vocab in our tokenizer : ' , len(tokenizer_for_bert.vocab) ) #BERT vocab has around 30K words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to encode text in format to feed to BERT\n",
    "\n",
    "def encode_text_for_bert (texts, tokenizer_for_bert, max_len=512):\n",
    "    ''' This function is to encode data for inputting into BERT model\n",
    "    Parameters:\n",
    "    texts - List of texts to encode\n",
    "    tokenizer_for_bert - Tokenizer to be used to convert text into tokens\n",
    "    max_len - Maximum length of text. It can have maximum value as 512\n",
    "    Return: Tupple of 3 numpy arrays \n",
    "    1) Token Ids padded with 0s to make length as max_len.  \n",
    "    2) Array where we have 1 for actual tokens and 0 for padding tokens\n",
    "    3) Array of 0s to indicate that token belongs to 1st sentence (chunk of text). There is no 2nd sentence here.\n",
    "    '''\n",
    "    all_token_ids = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer_for_bert.tokenize(text) #Tokenizing using Bert tokenizer\n",
    "            \n",
    "        tokens = tokens[:max_len-2] # Truncating number of tokens to max_len -2, Reduced extra 2 to add special tokens\n",
    "        \n",
    "        input_sequence = [\"[CLS]\"] + tokens + [\"[SEP]\"]  # [CLS] and [SEP] are special tokens to be added into input text\n",
    "        \n",
    "        pad_len = max_len - len(input_sequence) # Spaces to fill with 0s to make each sequence equal to max_len\n",
    "        \n",
    "        token_ids = tokenizer_for_bert.convert_tokens_to_ids(input_sequence)   #Converting tokens to token ids \n",
    "       \n",
    "        token_ids += [0] * pad_len  #Padding token ids with 0s\n",
    "        \n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len # 1 where we have sentence tokens and 0 otherwise\n",
    "        \n",
    "        segment_ids = [0] * max_len # Segment ids are all 0 to indicate it is part of sentence 1. There is no sentence 2 here\n",
    "        \n",
    "        all_token_ids.append(token_ids)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_token_ids), np.array(all_masks), np.array(all_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text after tokenization:  ['[CLS]', 'welcome', 'to', 'bert', 'session', '[SEP]']\n",
      "Test text after encoding:  (array([[  101,  6160,  2000, 14324,  5219,   102,     0]]), array([[1, 1, 1, 1, 1, 1, 0]]), array([[0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "#Optional Step: This is just to understand input/output of function encode_text_for_bert\n",
    "test_text =  \"Welcome to  BERT session \"\n",
    "\n",
    "print (\"Test text after tokenization: \" ,  [\"[CLS]\"] + tokenizer_for_bert.tokenize( test_text)  + [\"[SEP]\"] )\n",
    "\n",
    "print (\"Test text after encoding: \" ,encode_text_for_bert ( [test_text], tokenizer_for_bert, 7 ) ) # Pl Note id 101 is for token [CLS] and 102 for token [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating BERT  Model \n",
    "def bert_model_creation (bert_layer, max_len=512, model_type = 'Classification', num_classes = 2):\n",
    "    '''This function is to create BERT model for Classification or Regession Task\n",
    "    Parameters:\n",
    "    model_type = 'Classification' for classification task or 'Regression' for regression task. \n",
    "    num_classes = Number of classes in classification task. Value of 2 means binary classification. More than 2 for multiclass classification.\n",
    "                  For regression, num_classes parameter is ignored.\n",
    "    Return: Deep Learning Model\n",
    "    Important: You may add additional dense layers in place holder provided as \"***PLACEHOLDFER FOR ADDITIONAL LAYERS****\"\n",
    "    '''   \n",
    "    #Input to bert layer\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    #Output from bert layer\n",
    "    bert_layer_out = bert_layer([input_word_ids, input_mask, segment_ids]) # Python list of 2 tensors with shape (batch_size, 768) and (batch_size, max_len, 768)\n",
    "    \n",
    "    #Extrating Embedding for CLS token comming out of bert layer. Note CLS is the first token\n",
    "    cls_out = bert_layer_out[1][:,0,:] # Getting hidden-state of 1st tokens from second tensor in bert_layer_out, Tensor shape - (batch size, 768) \n",
    "    \n",
    "    \n",
    "    #***PLACEHOLDFER FOR ADDITIONAL LAYERS****. \n",
    "    #Add more layers here if you want. See example below\n",
    "    #cls_out = Dropout(.25) (cls_out)\n",
    "    #cls_out = Dense(500, activation='relu')(cls_out) \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Defines last layer depending on model type and  number of classes. Activation function is used depending on model_type and num_classes\n",
    "    if model_type == 'Classification' :\n",
    "        if num_classes == 2 :\n",
    "            out = Dense(1, activation='sigmoid')(cls_out)     # ** For Binary classification, use sigmoid activation\n",
    "        else:    \n",
    "            out = Dense(num_classes, activation='softmax')(cls_output) # For Multi Class classification, use softmax activation\n",
    "    else:\n",
    "        out = Dense(1, activation='linear')(cls_out)     # For regression, use linear activation\n",
    "    \n",
    "    #Model creation using inputs and output\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out, name='deeplearning_bert__model')\n",
    "    \n",
    "    \n",
    "    \n",
    "    learning_rate = 2e-6 # modify learning rate,as needed\n",
    "    \n",
    "    #Compiles Model depending on model type and number of classes. Loss function as well as metrics is used accordingly\n",
    "    if model_type == 'Classification' :\n",
    "        if num_classes == 2 :\n",
    "            model.compile(Adam(lr= learning_rate), loss='binary_crossentropy', metrics=['acc']) # ** For Binary classification\n",
    "        else:\n",
    "            model.compile(Adam(lr= learning_rate), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy']) # For Multi Class classification \n",
    "    else:\n",
    "        model.compile(Adam(lr= learning_rate), loss='mse', metrics=['mse']) # For Regression\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30 #Max length of text input to model. It can go up to 512. Keeping it small to run it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deeplearning_bert__model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_bert_layer (KerasLayer)   [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_bert_layer[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            769         tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Building Classification Model\n",
    "#modify values of model_type and num_classes as per need\n",
    "model = bert_model_creation(bert_layer, max_len=max_len, model_type = 'Classification', num_classes = 2) #binary classification as num_classes = 2\n",
    "\n",
    "#Model Summary. Pl note, there are ~109 Million parameters as it is BERT standard model\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Training Data for BERT.  If you want  preprocessing/cleaning of input text, it should be done before this step\n",
    "train_input = encode_text_for_bert(df_train['text'].values, tokenizer_for_bert, max_len= max_len)\n",
    "\n",
    "#Output variable as 0s or 1s for binary classification. It can have more distinct values for multi-class classification or continous values for regression\n",
    "y_train = df_train['target'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4264705882352941"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optional step: Checking accuracy on train tweets before fine-tuning so that we can see improvement by fine tuning\n",
    "accuracy_score( y_train, np.round(model.predict(train_input)).flatten() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5100 samples\n",
      "Epoch 1/2\n",
      "5100/5100 [==============================] - 79s 15ms/sample - loss: 0.5517 - acc: 0.7235\n",
      "Epoch 2/2\n",
      "5100/5100 [==============================] - 62s 12ms/sample - loss: 0.3968 - acc: 0.8278\n"
     ]
    }
   ],
   "source": [
    "#Model Training (Fine-tuning for tweets classification)\n",
    "epochs = 2       #Modify as neded\n",
    "batch_size = 32  #Modify as needed\n",
    "train_history = model.fit(train_input, y_train ,epochs= epochs,batch_size= batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572549019607844"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking accuracy on train tweets\n",
    "accuracy_score( y_train, np.round(model.predict(train_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8265021886191802"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding test data into BERT Format. If you have  preprocessing/cleaning of input text, it should be done before this step\n",
    "\n",
    "test_input = encode_text_for_bert(df_test['text'].values, tokenizer_for_bert, max_len= max_len)\n",
    "y_test = df_test['target'].values\n",
    "\n",
    "#Checking accuracy on test tweets. You may be able to improve it by taking bigger length of text, more epochs or by adding more dense layers into the model\n",
    "accuracy_score( y_test, np.round(model.predict(test_input)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Text:  We have a great data symposium today.\n",
      "Tweet Type:  [['Non Disaster']]    Score: [[0.0891896]]\n"
     ]
    }
   ],
   "source": [
    "#Running model on single text. Validating model for non-disaster text\n",
    "\n",
    "tweet = \"We have a great data symposium today.\"\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert ( [tweet], tokenizer_for_bert, max_len=max_len) ) \n",
    "print('Tweet Text: ', tweet)\n",
    "print ( 'Tweet Type: ', np.where(  prediction >= .5 , \"Disaster\", \"Non Disaster\" ) , '   Score:',  prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Text:  It is a terrorist attack. Take shelter  \n",
      "Tweet Type:  [['Disaster']]    Score: [[0.6896698]]\n"
     ]
    }
   ],
   "source": [
    "#Running model on single text. Validating model for disaster text\n",
    "\n",
    "tweet = \"It is a terrorist attack. Take shelter  \"\n",
    "\n",
    "prediction = model.predict (  encode_text_for_bert ( [tweet], tokenizer_for_bert, max_len=max_len) ) \n",
    "print('Tweet Text: ', tweet)\n",
    "print ( 'Tweet Type: ', np.where(  prediction >= .5 , \"Disaster\", \"Non Disaster\" ) , '   Score:',  prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
